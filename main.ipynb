{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e9b019-5baa-4623-bb5b-405c35130db2",
   "metadata": {},
   "source": [
    "# Movie Revenues and IMDb data pipeline prototype\n",
    "\n",
    "This notebook presents my idea for a pipeline that ingests `revenue_per_day csv` file, enriching it with data from the OMDB API (IMDb).\n",
    "Despite the fact, that a real pipeline should be built on the basis of other tools than SQLite and Python with pandas and should not be *orchestrated* by a notebook. Nevertheless, I decided on such a solution because, despite some simplifications, it is also able to show my process of thinking, development and approaching problems in accessible way.\n",
    "\n",
    "I also decided to implement the data presentation process in a notebook using python libraries because I currently do not have any tool in which I would be able to quickly prepare dashboards. I didnt manage to find that much of time to recognize available dedicated OpenSource solutions in this area.\n",
    "\n",
    "Initially I wanted to do more in Pandas and SQLALchemy but executing statements even in SQLite are significantly faster. Some SQL statements can look messy and overwhelming. It is partially due to the my choice - SQLite - and its limitations. For example it doesn't have the UDF. Of course statements could be done better but it would cost more involvement and time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41170be4-426c-4714-8baf-f3d55ee53c95",
   "metadata": {},
   "source": [
    "## Data model\n",
    "The code below implements the data model whose definition can be found in the file `./diagrams/diagram.dbml` (can be recreated in `https://dbdiagram.io/`) and which I present below:\n",
    "\n",
    "![Cat](pics/diagram_screenshot.png)\n",
    "\n",
    "The model consists of two STG tables:\n",
    "- `stg_revenues_per_day`\n",
    "- `stg_movies_details`\n",
    "\n",
    "five dimensions DWH tables:\n",
    "- `dwh_dim__movies_reviewers`\n",
    "- `dwh_dim__movies`\n",
    "- `dwh_dim__reviews_results`\n",
    "- `dwh_dim__distributors`\n",
    "- `dwh_dim__distributors`\n",
    "- `dwh_dim__dates`\n",
    "\n",
    "and one fact DWH table:\n",
    "- `dwh_fact__revenues`.\n",
    "\n",
    "The tables have prefixes dwh_ and stg_ to mimic `schema_name.table_name`. Ideally the should be splited between two schemas but SQLIte doesn't use schemas division concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b0e10-b329-48b5-806d-2f2171e3e6d9",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "Pipeline is divided into few task groups (using the Airflow nomenclature). Arrows ~~>~~ shows the direction of the steps.\n",
    "\n",
    "\n",
    "![Cat](pics/main_flow.png)\n",
    "\n",
    "\n",
    "The pictures shows only idea in about order of execution of actions. Particular tasks will be represent below as next steps with usage of proper functions. It is crucial to execute them in the order presented in the notebook. Solution presented in this notebook doesn't have implemented any mechanism to define strict dependencies. \n",
    "\n",
    "In most cases, tasks will be represented as Python functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148b0d6-45e5-48a1-8b81-d60238fb20d5",
   "metadata": {},
   "source": [
    "### main.start\n",
    "\n",
    "Starts and stops are placeholders like EmptyOpertator in Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2599e-0063-4bc8-b663-b01a24e8e9c2",
   "metadata": {},
   "source": [
    "### main.creation\n",
    "This function iterates through a lists of ORM models defined in `models.schemas.schema`. It creates their corresponding tables in the database and sets up triggers. In the proper pipeline actions can be executed parallelly.\n",
    "\n",
    "![Cat](pics/creation_flow.png)\n",
    "\n",
    "**All tables have technical fields `created_date` and `modified_date`. All DWH tables have additionally `id` as primary key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e1cb1a-c312-4c8e-8c2c-1705a2cc5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlmodel import create_engine\n",
    "from pipeline.creation import create_from_orms\n",
    "from models.schemas.schema import stg_orms, dwh_orms\n",
    "\n",
    "engine = create_engine(f\"sqlite:///./db/task.db\", echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8011d09f-7411-4bcf-8d83-51246f9095fa",
   "metadata": {},
   "source": [
    "#### creation.create_stg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce9ec0a-8d4b-481f-87a4-187ff78a08be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If not existed, table 'stg_revenues_per_day' created + modified_date trigger\n",
      "If not existed, table 'stg_movies_details' created + modified_date trigger\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<State.SUCCESS: 'SUCCESS'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_from_orms(models=stg_orms, engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe3153-6087-4573-a613-c4064adc5743",
   "metadata": {},
   "source": [
    "#### creation.create_dwh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e610b3-6379-48f2-9af3-0d376644103a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If not existed, table 'dwh_dim__movies_reviewers' created + modified_date trigger\n",
      "If not existed, table 'dwh_dim__movies' created + modified_date trigger\n",
      "If not existed, table 'dwh_dim__reviews_results' created + modified_date trigger\n",
      "If not existed, table 'dwh_dim__distributors' created + modified_date trigger\n",
      "If not existed, table 'dwh_dim__dates' created + modified_date trigger\n",
      "If not existed, table 'dwh_fact__revenues' created + modified_date trigger\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<State.SUCCESS: 'SUCCESS'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_from_orms(models=dwh_orms, engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109fc443-4dcb-4e66-8a2a-1dc99fa25ecb",
   "metadata": {},
   "source": [
    "### main.ingestion\n",
    "\n",
    "Here are two tasks. They have to be executed in series. Data gathered in the first step is used in next to do API calls.\n",
    "\n",
    "![Cat](pics/ingestion_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213bdafe-578e-4d45-a1ea-9e7327d35c88",
   "metadata": {},
   "source": [
    "#### ingestion.ingest_revenues\n",
    "\n",
    "This function loads a CSV file into memory as a Pandas DataFrame based on the parameters defined in the `CSVIngesterDefinition` object (`revenues_per_day_definition`). From DataFrame the temp table is created in DB. Using the temp table there are UPSERTs to proper `stg_revenues_per_day`. In seperate session the temp table is dropped.\n",
    "\n",
    "Initially I introduced more elegant version with SQLAlchemy merge but it turned out to be sinificantly slower than temp table and UPSERT. I left the old ingester in files because it is more generic and could be used to load data to different tables with ORM classes.\n",
    "Fn `csv_ingester_revenues` used here is tailored for the specific table and specific CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "874492f7-9fee-424e-851d-23f07d3c13a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded to memory. Starting db merge\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<State.SUCCESS: 'SUCCESS'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pipeline.ingestion import csv_ingester_revenues\n",
    "from models.definitions.ingestion import revenues_per_day_definition\n",
    "\n",
    "csv_ingester_revenues(definition=revenues_per_day_definition, engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d53f96-e372-4bdb-8348-a88a8090c69f",
   "metadata": {},
   "source": [
    "#### ingestion.fetch_and_ingest_movie_details\n",
    "\n",
    "This function retrieves a list of distinct movie titles from the database, queries the OMDB API for details, and stores the results in a temp table. In case of faulty responses or API errors, it keeps track of the number of failed attempts and stops once a certain threshold is reached. The fetched data is merged into the main table, and the temporary table is dropped after successful execution.\n",
    "The `OMDBAPIFetchDefinition` object (`movies_details_api_fetch_definition`) stoers parameters which define the API (url, faulty attempts number, dry run)\n",
    "\n",
    "Initially I introduced more elegant version with SQLAlchemy merge but it turned out to be sinificantly slower than temp table and UPSERT.\n",
    "`Movie not found!` this error is not taken into account when calculating the number of incorrect attempts.\n",
    "\n",
    "**You can limit the api calls for test purposes using `limit_calls`** <br>\n",
    "`OMDB_API_KEY` valid must be passed to function. **Below you can put your API key as env varaible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db4e35-26eb-4d85-a4d7-0508d79e8c3d",
   "metadata": {},
   "source": [
    "**\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/**  `OMDB_API_KEY`  **\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1cd72c4-6711-4c2f-bebc-fde53dd71df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMDB_API_KEY=TO_BE_DEFINED!!\n"
     ]
    }
   ],
   "source": [
    "%env OMDB_API_KEY=TO_BE_DEFINED!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466c08f-a70a-479e-a2d1-43de2f63dcc5",
   "metadata": {},
   "source": [
    "**\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/**  `OMDB_API_KEY`  **\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9586f304-46a8-49af-88cf-1f09cc5e54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pipeline.ingestion import fetch_movies_details\n",
    "from models.definitions.ingestion import movies_details_api_fetch_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c2d11a2-07cb-4bbd-800a-3cde6e977bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▌                                                                                                                                                                                             | 33/2509 [00:02<03:41, 11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faulty API response for Poku00e9mon the Movie 2000. Movie not found!\n",
      "Faulty API response for Poku00e9mon 3 the Movie: Spell of the Unown. Request limit reached!\n",
      "Faulty API response for Amu00e9lie. Movie not found!\n",
      "Faulty API response for Beauty and the Beast2000 IMAX Release. Movie not found!\n",
      "Faulty API response for Peter Pan 2: Return to Never Land. Movie not found!\n",
      "Faulty API response for E.T. the Extra-Terrestrial20th Anniversary. Request limit reached!\n",
      "Faulty API response for Van Wilder: Party Liaison. Request limit reached!\n",
      "Faulty API response for Spider-Man/Men in Black IIDouble Bill. Request limit reached!\n",
      "Faulty API response for Lawrence of Arabia2002 Re-release. Request limit reached!\n",
      "Faulty API response for Star Wars: Episode II - Attack of the Clones2002 IMAX Release. Movie not found!\n",
      "Faulty API response for Singin' in the Rain2002 Re-release. Movie not found!\n",
      "Faulty API response for The Lion King2002 IMAX Release. Movie not found!\n",
      "Faulty API response for He Loves Me... He Loves Me Not. Movie not found!\n",
      "Faulty API response for Talk to Her2002 Re-release. Request limit reached!\n",
      "Faulty API response for Winged Migration2003 Re-release. Movie not found!\n",
      "Faulty API response for L'auberge espagnole2003 Re-release. Movie not found!\n",
      "Faulty API response for Poku00e9mon Heroes. Movie not found!\n",
      "Faulty API response for Lucu00eda, Lucu00eda. Movie not found!\n",
      "Faulty API response for Spy Kids 3-D: Game Over. Movie not found!\n",
      "Faulty API response for The Magdalene Sisters2003 Re-release. Movie not found!\n",
      "Faulty API response for Xi yang tian shi. Movie not found!\n",
      "Faulty API response for Mambo Italiano2003 Re-release. Movie not found!\n",
      "Faulty API response for The Visual Bible: The Gospel of John. Movie not found!\n",
      "Faulty API response for Scarface2003 Re-release. Movie not found!\n",
      "Faulty API response for Km. 0 - Kilometer Zero. Movie not found!\n",
      "Faulty API response for Alien2003 Director's Cut. Movie not found!\n",
      "Faulty API response for The Barbarian Invasions2003 Re-release. Request limit reached!\n",
      "Faulty API response for The Lord of the Rings: The Fellowship of the Ring2003 Extended Edition. Movie not found!\n",
      "Faulty API response for The Lord of the Rings: The Two Towers2003 Re-release. Request limit reached!\n",
      "Faulty API response for The Fog of War: Eleven Lessons from the Life of Robert S. McNamara. Request limit reached!\n",
      "Faulty API response for Donnie Darko2004 Director's Cut. Request limit reached!\n",
      "Faulty API response for Taxi2004 Re-release. Movie not found!\n",
      "Faulty API response for Raging Bull25th Anniversary. Request limit reached!\n",
      "The number of faulty API responses exceeded the allowance. Finishing calling API\n",
      "API calls finished. 0 entries fetched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<State.SUCCESS: 'SUCCESS'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_movies_details(definition=movies_details_api_fetch_definition,\n",
    "                     api_key=os.environ.get('OMDB_API_KEY'),\n",
    "                     engine=engine,\n",
    "                     limit_calls=10000\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb36a2-d21a-48d1-8c99-00cb1839888e",
   "metadata": {},
   "source": [
    "### main.transformation\n",
    "\n",
    "The transformation of data and population of DWH tables is done by executing SQL statements which are stored in `./pipeline/transformation` directory.  \n",
    "\n",
    "![Cat](pics/transformation_flow.png)\n",
    "\n",
    "All statements are written as UPSERTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2ffd26-2061-4444-81b5-532659f83283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.transformation import populate_using_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683247e3-d35b-4e26-930b-0567130ecac7",
   "metadata": {},
   "source": [
    "#### transformation.populate_dim__dates\n",
    "This statement collects all distinct dates from STG sources and transform them into format `YYYY-MM-DD`.\n",
    "\n",
    "**The conversion from format `DD MMM YYYY` is pretty nasty due to the SQLite limitations in that matter!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "436a3ffc-a5a2-4c5f-b998-034fb397a4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<State.SUCCESS: 'SUCCESS'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "populate_using_sql(filename=\"dwh_dim__dates.sql\", engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2a551-51b0-45e6-94e4-73c0cd42aaf5",
   "metadata": {},
   "source": [
    "#### transformation.populate_dim__distributors\n",
    "This statement collects all distinct movie distributors. The value `-` is considered as `NULL`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e9b53a-b682-419d-b753-5cdf2c8a73e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<State.SUCCESS: 'SUCCESS'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "populate_using_sql(filename=\"dwh_dim__distributors.sql\", engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8d1dd-39fd-40d0-aec3-1a00b93cbd8c",
   "metadata": {},
   "source": [
    "#### transformation.populate_dim__movies_reviewers\n",
    "This statement collects all distinct entities which are reviewing and scoring movies. Additinal INSERT is with `IMDb` as reviewer. Originally the IMDb scores are not included in `Ratings` but I decided to unify it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d9b710d-6749-4dc3-a88c-c3677aab770f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<State.SUCCESS: 'SUCCESS'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "populate_using_sql(filename=\"dwh_dim__movies_reviewers.sql\", engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676daeac-d9e1-4497-b6ea-e7c10eb8c268",
   "metadata": {},
   "source": [
    "#### transformation.populate_dim__movies\n",
    "The step with biggest number of changes in data. Data is taken from JSON stored in STG table.\n",
    "- `Year` field is splited (if it consist of span of years like `2001-2005` into two. In proper `YYYY-MM-DD` format and stored as relation to dim table `start_year_date_id` and `end_year_date_id`. The year is represented by first day of year so for example 2001 => 2001-01-01.\n",
    "- `Rated` and `type` is set as Enum in ORM.\n",
    "- `Released` and `DVD` transformed in proper `YYYY-MM-DD` format and stoared as relation to dim table as `release_date_id`, `dvd_release_date_id` .\n",
    "- `Genre`, `Director`, `Writer` and `Actors`, `Language`, `Country` are transformed into JSON arrays. <br><br>\n",
    "**The conversion from format `DD MMM YYYY` is pretty nasty due to the SQLite limitations in that matter!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a38292-852d-407d-95c7-5c03e96ff2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<State.SUCCESS: 'SUCCESS'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "populate_using_sql(filename=\"dwh_dim__movies.sql\", engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a091f1df-7bea-4f4f-9ed6-c09a69e7a023",
   "metadata": {},
   "source": [
    "#### transformation.populate_dim__reviews_results\n",
    "All ratings are transformed into percent value from 0-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaf80abf-6681-498b-9edc-298baedc8ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<State.SUCCESS: 'SUCCESS'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "populate_using_sql(filename=\"dwh_dim__reviews_results.sql\", engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75090a4f-4d36-4e04-aa84-4785cd95efeb",
   "metadata": {},
   "source": [
    "#### transformation.populate_fact__revenues\n",
    "In the end the fact table is populated with 3 foreign keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "777c6b19-e5aa-4084-927f-fa24b7f7c58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<State.SUCCESS: 'SUCCESS'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "populate_using_sql(filename=\"dwh_fact__revenues.sql\", engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3f7ee-1038-4d04-afb7-0ba65a941928",
   "metadata": {},
   "source": [
    "### main.create_views\n",
    "\n",
    "Create views function is similar to `populate_using_sql` and in this case (for now) all can be run paralelly. There is no dependency. Therefore I didn't add the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f61b713-6d14-4692-bc5e-313d457e9277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<State.SUCCESS: 'SUCCESS'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pipeline.create_views import create_using_sql\n",
    "\n",
    "create_using_sql(filename=\"actors_revenues.sql\", engine=engine)\n",
    "create_using_sql(filename=\"countries_revenues.sql\", engine=engine)\n",
    "create_using_sql(filename=\"directors_revenues.sql\", engine=engine)\n",
    "create_using_sql(filename=\"movies_revenues.sql\", engine=engine)\n",
    "create_using_sql(filename=\"per_genre_revenues.sql\", engine=engine)\n",
    "create_using_sql(filename=\"per_month_revenues.sql\", engine=engine)\n",
    "create_using_sql(filename=\"per_rating_revenues.sql\", engine=engine)\n",
    "create_using_sql(filename=\"per_year_revenues.sql\", engine=engine)\n",
    "create_using_sql(filename=\"writers_revenues.sql\", engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c0623d-6918-45e1-bd8e-6d42256a453e",
   "metadata": {},
   "source": [
    "### main.stop\n",
    "\n",
    "This is the end of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d7d5b-6326-42f4-8bdb-04c624f7f1ff",
   "metadata": {},
   "source": [
    "## Dasboard\n",
    "The graph shows total revenues (average for dates) with the dropdown which allows to change the context on the fly. Please bear in mind that it's still about the revenues of the movies, not individual units. \n",
    "Example: if Daniel Radcliffe, the actor playing Harry Potter is in first place, it doesn't mean that he's the richest, but that the films in which he played turned out to be the most profitable (in dataset).\n",
    "\n",
    "**If you don't have time to run all commands and fetch significant amount of data from API I left the little demo pics in `./pics/` with `demo_` prefix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05087087-b233-4439-855d-4705fdc3086b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a36232360247c9b4d6435059279dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Select', index=1, options=(('Month', 'per_month'), ('Genres', 'gen…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets\n",
    "from ipywidgets.widgets import fixed\n",
    "from models.definitions.dashboard import dropdown, most_successful_plots_definitions\n",
    "from pipeline.dashboard.successful import get_most_successful_graph\n",
    "\n",
    "interactive_output = ipywidgets.interactive(get_most_successful_graph,\n",
    "                                            selected=dropdown,\n",
    "                                            engine=fixed(engine),\n",
    "                                            definition=fixed(most_successful_plots_definitions))\n",
    "display(interactive_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c128e-c171-45e1-b416-9f7e066375e7",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Some records were downloaded with different names than they were originally in CSV. This is due to the API bug or feature, which, for example, when getting the position \"Sarnie Zniwo\" returns \"Sarnie zniwo, czyli Pokusa statuetkowego szlaku\" which is good but in some cases causes incorrect operation, e.g. for the title \"Together With You\" it downloaded \"A Christmas Together with You\" and I assume it should be \"Together With You (Well, If We Get Along)\". A query with these inconsistencies should be prepared and the data bringed into compliance (even manually).\n",
    "\n",
    "- Some titles from CSV are corrupted or encoded in a way that the API does not recognize them properly. These names should also be caught (even manually) and corrected.\n",
    "\n",
    "- \"any_ingester\" which was left in the code but will not conduct db merge well for the current models due to the `created_date` and `updated_date` fields that I introduced later. This function would always update the records. To restore its correct operation, two models would have to be created for each table.\n",
    "\n",
    "- Perhaps data for generating the chart should be cached in memory because if there were more of them it could cause slow loading.\n",
    "\n",
    "- My main goal was to prepare plot which is somehow interactive. There was plans to prepare more but the weekend ended :( . It is pity that I didn't used the normalized scores somehow in context of revenues/box office. The plots with number of theaters also would be interesting.\n",
    "\n",
    "- Thank you for the task. Very cool. I had fun doing it. Of course I would still improve many things in it despite the \"primitive\" tools.\n",
    "\n",
    "- If Michael Bay would direct the new Harry Potter with Daniel Radcliffe still in the lead role, and the premiere would be in the middle of year the film would be financial hit :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task",
   "language": "python",
   "name": "task"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
